{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4f1eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce RTX 3050 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "#This script is generated using AI.\n",
    "#Reason: Because of dataset being large, it was difficult to run it on CPU. Cloud GPU had a limit of 3 hours which was insufficent to process this dataset.\n",
    "#SO we utilized local GPU on one of our machines to process the dataset.\n",
    "\n",
    "import torch\n",
    "print(torch.cuda.is_available())           # Should return True\n",
    "print(torch.cuda.get_device_name(0))       # Should return your GPU name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97c4967",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 586714it [5:52:51, 27.71it/s] \n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import hashlib\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load model and tokenizer\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Adjustable batch size\n",
    "PREDICTION_BATCH_SIZE = 64  # Increase if you have enough GPU memory\n",
    "READ_BATCH_SIZE = 500       # Number of records to read from input before predicting\n",
    "\n",
    "# Define dataset and prediction function\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding=True, max_length=max_length, return_tensors='pt')\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.encodings['input_ids'].shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}\n",
    "\n",
    "def predict_sentiment_batch(texts):\n",
    "    dataset = TextDataset(texts, tokenizer)\n",
    "    dataloader = DataLoader(dataset, batch_size=PREDICTION_BATCH_SIZE)\n",
    "    sentiment_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "            preds = torch.argmax(probs, dim=-1)\n",
    "            predictions.extend([sentiment_map[p.item()] for p in preds])\n",
    "\n",
    "    return predictions\n",
    "\n",
    "# File paths\n",
    "input_path = 'dataset_df.json'\n",
    "output_path = 'sentiment_output_streamed.json'\n",
    "\n",
    "# Step 1: Load already-processed line hashes\n",
    "existing_hashes = set()\n",
    "try:\n",
    "    with open(output_path, 'r', encoding='utf-8') as outf:\n",
    "        for line in outf:\n",
    "            record = json.loads(line)\n",
    "            text_hash = hashlib.md5(record['text'].encode('utf-8')).hexdigest()\n",
    "            existing_hashes.add(text_hash)\n",
    "except FileNotFoundError:\n",
    "    pass  # New file will be created\n",
    "\n",
    "# Step 2: Stream and skip already processed lines\n",
    "batch_texts = []\n",
    "batch_records = []\n",
    "\n",
    "with open(input_path, 'r', encoding='utf-8') as infile, open(output_path, 'a', encoding='utf-8') as outfile:\n",
    "    for line in tqdm(infile, desc=\"Processing\"):\n",
    "        line = line.strip()\n",
    "        if not line:\n",
    "            continue\n",
    "        try:\n",
    "            record = json.loads(line)\n",
    "            text = record[\"text\"]\n",
    "            text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()\n",
    "            if text_hash in existing_hashes:\n",
    "                continue  # Skip already processed line\n",
    "\n",
    "            batch_texts.append(text)\n",
    "            batch_records.append(record)\n",
    "        except json.JSONDecodeError:\n",
    "            continue\n",
    "\n",
    "        if len(batch_texts) >= READ_BATCH_SIZE:\n",
    "            sentiments = predict_sentiment_batch(batch_texts)\n",
    "            for record, sentiment in zip(batch_records, sentiments):\n",
    "                record[\"sentiment\"] = sentiment\n",
    "                json.dump(record, outfile, ensure_ascii=False)\n",
    "                outfile.write('\\n')\n",
    "            batch_texts.clear()\n",
    "            batch_records.clear()\n",
    "\n",
    "    # Process leftover records\n",
    "    if batch_texts:\n",
    "        sentiments = predict_sentiment_batch(batch_texts)\n",
    "        for record, sentiment in zip(batch_records, sentiments):\n",
    "            record[\"sentiment\"] = sentiment\n",
    "            json.dump(record, outfile, ensure_ascii=False)\n",
    "            outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8cf415b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "data = []\n",
    "with open('full_data\\\\sentiment_output_streamed.json', 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:  # skip blank lines\n",
    "            try:\n",
    "                data.append(json.loads(line))\n",
    "            except json.JSONDecodeError:\n",
    "                print(\"Skipping invalid line:\", line)\n",
    "sentiment_df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd19d5db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>business_id</th>\n",
       "      <th>stars</th>\n",
       "      <th>useful</th>\n",
       "      <th>funny</th>\n",
       "      <th>cool</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>name</th>\n",
       "      <th>review_count</th>\n",
       "      <th>categories</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>state</th>\n",
       "      <th>postal_code</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QzCEzH3R7Z6erOGLr3t55Q</td>\n",
       "      <td>0pMj5xUAecW9o1P35B0AMw</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Great staff always helps and always nice. Alwa...</td>\n",
       "      <td>2017-05-26 13:10:24</td>\n",
       "      <td>Wawa</td>\n",
       "      <td>8</td>\n",
       "      <td>Food, Coffee &amp; Tea, Gas Stations, Restaurants,...</td>\n",
       "      <td>2544 W Main Street</td>\n",
       "      <td>Norristown</td>\n",
       "      <td>PA</td>\n",
       "      <td>19403</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3MpDvy5gEdsbZh9-p92dHg</td>\n",
       "      <td>8QnuWGVNBhzyYXGSeRdi4g</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>After my ROTD  yesterday of a different Sweet ...</td>\n",
       "      <td>2013-10-24 19:24:33</td>\n",
       "      <td>Sweet Cece's</td>\n",
       "      <td>17</td>\n",
       "      <td>Food, Ice Cream &amp; Frozen Yogurt</td>\n",
       "      <td>7114 Hwy 70 S</td>\n",
       "      <td>Nashville</td>\n",
       "      <td>TN</td>\n",
       "      <td>37221</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bCla27ma_6i_QFrGkILKrQ</td>\n",
       "      <td>sLgnx_WFCjEoPsS6NwU70Q</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Our family returned for breakfast again this w...</td>\n",
       "      <td>2014-10-27 16:31:37</td>\n",
       "      <td>Le Peep</td>\n",
       "      <td>259</td>\n",
       "      <td>Event Planning &amp; Services, Salad, Caterers, Am...</td>\n",
       "      <td>3036 N Eagle Rd</td>\n",
       "      <td>Meridian</td>\n",
       "      <td>ID</td>\n",
       "      <td>83646</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>UsBxLh14sUpO8SdeqIiGOA</td>\n",
       "      <td>Wy8Hswf2cLQGRZN6armkag</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>If I could give it a zero, I would. I order a ...</td>\n",
       "      <td>2011-08-24 23:07:08</td>\n",
       "      <td>Jack in the Box</td>\n",
       "      <td>86</td>\n",
       "      <td>Restaurants, Fast Food, Mexican, Tacos, Burger...</td>\n",
       "      <td>6875 Hollister Ave</td>\n",
       "      <td>Goleta</td>\n",
       "      <td>CA</td>\n",
       "      <td>93117</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mEOMAeEonZoUx2nPM3v6fg</td>\n",
       "      <td>f-WhNOSwN1aB4nRFekf01g</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Id you haven't been to the Smoothie King cente...</td>\n",
       "      <td>2015-03-19 00:30:09</td>\n",
       "      <td>Smoothie King</td>\n",
       "      <td>50</td>\n",
       "      <td>Arts &amp; Entertainment, Ticket Sales, Food, Juic...</td>\n",
       "      <td>1501 Dave Dixon Drive Space 101-102</td>\n",
       "      <td>New Orleans</td>\n",
       "      <td>LA</td>\n",
       "      <td>70113</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586656</th>\n",
       "      <td>6YRmtPVvnD_mexQifxAJsg</td>\n",
       "      <td>IkjBNJvMZhea1c3j2H2Ahw</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Besides it being super busy often, usually, I ...</td>\n",
       "      <td>2022-01-10 12:51:07</td>\n",
       "      <td>Starbucks</td>\n",
       "      <td>31</td>\n",
       "      <td>Food, Coffee &amp; Tea</td>\n",
       "      <td>2350 South Grand Ave</td>\n",
       "      <td>St. Louis</td>\n",
       "      <td>MO</td>\n",
       "      <td>63104</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586657</th>\n",
       "      <td>6Hmh8UC5K0oBgp3_D5doDQ</td>\n",
       "      <td>BxveKq0rKp52EWooIDK54w</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Where do i begin. Overall the food was fine. I...</td>\n",
       "      <td>2019-12-08 19:24:12</td>\n",
       "      <td>Denny's</td>\n",
       "      <td>88</td>\n",
       "      <td>Breakfast &amp; Brunch, American (Traditional), Re...</td>\n",
       "      <td>3155 E Fairview Ave</td>\n",
       "      <td>Meridian</td>\n",
       "      <td>ID</td>\n",
       "      <td>83642</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586658</th>\n",
       "      <td>UfevNSM_H14XXWZFlHYPoA</td>\n",
       "      <td>TNtcjnta11CpDebuBNdoug</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Our waiter was good but a bit rushed. I didn't...</td>\n",
       "      <td>2017-10-22 20:55:12</td>\n",
       "      <td>Red Robin Gourmet Burgers and Brews</td>\n",
       "      <td>83</td>\n",
       "      <td>Home Services, American (New), Contractors, Am...</td>\n",
       "      <td>130 Gravois Bluffs Cir</td>\n",
       "      <td>Fenton</td>\n",
       "      <td>MO</td>\n",
       "      <td>63026</td>\n",
       "      <td>Negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586659</th>\n",
       "      <td>LHWtjTG7e1NzNPYUbUo-9w</td>\n",
       "      <td>rgeuy1qbw6Z8B6CSVANHIA</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>I've been to the other Federal Donuts location...</td>\n",
       "      <td>2012-10-13 14:39:37</td>\n",
       "      <td>Federal Donuts</td>\n",
       "      <td>1464</td>\n",
       "      <td>Donuts, Sandwiches, Soul Food, Food, Coffee &amp; ...</td>\n",
       "      <td>1632 Sansom St</td>\n",
       "      <td>Philadelphia</td>\n",
       "      <td>PA</td>\n",
       "      <td>19103</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>586660</th>\n",
       "      <td>x1QLCwZGFAjxRRw4EHc3-g</td>\n",
       "      <td>1_BVWDzi5cVqWxNe9bOMMQ</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Don't misinterpret my 5-star review....I don't...</td>\n",
       "      <td>2016-04-30 01:02:34</td>\n",
       "      <td>Wendy's</td>\n",
       "      <td>55</td>\n",
       "      <td>Restaurants, Burgers, Food, Fast Food</td>\n",
       "      <td>2130 Hampton Avenue</td>\n",
       "      <td>St.Louis</td>\n",
       "      <td>MO</td>\n",
       "      <td>63139</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>586661 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       user_id             business_id  stars  useful  funny  \\\n",
       "0       QzCEzH3R7Z6erOGLr3t55Q  0pMj5xUAecW9o1P35B0AMw    5.0       1      0   \n",
       "1       3MpDvy5gEdsbZh9-p92dHg  8QnuWGVNBhzyYXGSeRdi4g    4.0       0      0   \n",
       "2       bCla27ma_6i_QFrGkILKrQ  sLgnx_WFCjEoPsS6NwU70Q    5.0       0      0   \n",
       "3       UsBxLh14sUpO8SdeqIiGOA  Wy8Hswf2cLQGRZN6armkag    1.0       1      0   \n",
       "4       mEOMAeEonZoUx2nPM3v6fg  f-WhNOSwN1aB4nRFekf01g    4.0       0      0   \n",
       "...                        ...                     ...    ...     ...    ...   \n",
       "586656  6YRmtPVvnD_mexQifxAJsg  IkjBNJvMZhea1c3j2H2Ahw    3.0       0      0   \n",
       "586657  6Hmh8UC5K0oBgp3_D5doDQ  BxveKq0rKp52EWooIDK54w    2.0       3      1   \n",
       "586658  UfevNSM_H14XXWZFlHYPoA  TNtcjnta11CpDebuBNdoug    2.0       0      1   \n",
       "586659  LHWtjTG7e1NzNPYUbUo-9w  rgeuy1qbw6Z8B6CSVANHIA    5.0       1      1   \n",
       "586660  x1QLCwZGFAjxRRw4EHc3-g  1_BVWDzi5cVqWxNe9bOMMQ    5.0       1      0   \n",
       "\n",
       "        cool                                               text  \\\n",
       "0          1  Great staff always helps and always nice. Alwa...   \n",
       "1          0  After my ROTD  yesterday of a different Sweet ...   \n",
       "2          0  Our family returned for breakfast again this w...   \n",
       "3          0  If I could give it a zero, I would. I order a ...   \n",
       "4          0  Id you haven't been to the Smoothie King cente...   \n",
       "...      ...                                                ...   \n",
       "586656     0  Besides it being super busy often, usually, I ...   \n",
       "586657     0  Where do i begin. Overall the food was fine. I...   \n",
       "586658     0  Our waiter was good but a bit rushed. I didn't...   \n",
       "586659     1  I've been to the other Federal Donuts location...   \n",
       "586660     1  Don't misinterpret my 5-star review....I don't...   \n",
       "\n",
       "                       date                                 name  \\\n",
       "0       2017-05-26 13:10:24                                 Wawa   \n",
       "1       2013-10-24 19:24:33                         Sweet Cece's   \n",
       "2       2014-10-27 16:31:37                              Le Peep   \n",
       "3       2011-08-24 23:07:08                      Jack in the Box   \n",
       "4       2015-03-19 00:30:09                        Smoothie King   \n",
       "...                     ...                                  ...   \n",
       "586656  2022-01-10 12:51:07                            Starbucks   \n",
       "586657  2019-12-08 19:24:12                              Denny's   \n",
       "586658  2017-10-22 20:55:12  Red Robin Gourmet Burgers and Brews   \n",
       "586659  2012-10-13 14:39:37                       Federal Donuts   \n",
       "586660  2016-04-30 01:02:34                              Wendy's   \n",
       "\n",
       "        review_count                                         categories  \\\n",
       "0                  8  Food, Coffee & Tea, Gas Stations, Restaurants,...   \n",
       "1                 17                    Food, Ice Cream & Frozen Yogurt   \n",
       "2                259  Event Planning & Services, Salad, Caterers, Am...   \n",
       "3                 86  Restaurants, Fast Food, Mexican, Tacos, Burger...   \n",
       "4                 50  Arts & Entertainment, Ticket Sales, Food, Juic...   \n",
       "...              ...                                                ...   \n",
       "586656            31                                 Food, Coffee & Tea   \n",
       "586657            88  Breakfast & Brunch, American (Traditional), Re...   \n",
       "586658            83  Home Services, American (New), Contractors, Am...   \n",
       "586659          1464  Donuts, Sandwiches, Soul Food, Food, Coffee & ...   \n",
       "586660            55              Restaurants, Burgers, Food, Fast Food   \n",
       "\n",
       "                                    address          city state postal_code  \\\n",
       "0                        2544 W Main Street    Norristown    PA       19403   \n",
       "1                             7114 Hwy 70 S     Nashville    TN       37221   \n",
       "2                           3036 N Eagle Rd      Meridian    ID       83646   \n",
       "3                        6875 Hollister Ave        Goleta    CA       93117   \n",
       "4       1501 Dave Dixon Drive Space 101-102   New Orleans    LA       70113   \n",
       "...                                     ...           ...   ...         ...   \n",
       "586656                 2350 South Grand Ave     St. Louis    MO       63104   \n",
       "586657                  3155 E Fairview Ave      Meridian    ID       83642   \n",
       "586658               130 Gravois Bluffs Cir        Fenton    MO       63026   \n",
       "586659                       1632 Sansom St  Philadelphia    PA       19103   \n",
       "586660                  2130 Hampton Avenue      St.Louis    MO       63139   \n",
       "\n",
       "       sentiment  \n",
       "0       Positive  \n",
       "1       Positive  \n",
       "2       Positive  \n",
       "3       Negative  \n",
       "4       Positive  \n",
       "...          ...  \n",
       "586656  Positive  \n",
       "586657  Negative  \n",
       "586658  Negative  \n",
       "586659  Positive  \n",
       "586660  Positive  \n",
       "\n",
       "[586661 rows x 16 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84dbaafe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the mapping from sentiment labels to scores\n",
    "sentiment_score_map = {\n",
    "    \"Negative\": 1,\n",
    "    \"Neutral\": 2,\n",
    "    \"Positive\": 3\n",
    "}\n",
    "\n",
    "# Apply the mapping to create a new column\n",
    "sentiment_df[\"sentiment_score\"] = sentiment_df[\"sentiment\"].map(sentiment_score_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dda42f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_3_scale(score):\n",
    "    if score in [1, 2]:\n",
    "        return 1\n",
    "    elif score == 3:\n",
    "        return 2\n",
    "    elif score in [4, 5]:\n",
    "        return 3\n",
    "\n",
    "# Apply to both columns\n",
    "sentiment_df[\"stars_3_scale\"] = sentiment_df[\"stars\"].apply(convert_to_3_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "948b1325",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_df.to_json('sentiment_score.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "db22af29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact Match Accuracy: 0.8222619195753595\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(sentiment_df[\"stars_3_scale\"].astype(int),\n",
    "                          sentiment_df[\"sentiment_score\"].astype(int))\n",
    "print(\"Exact Match Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
